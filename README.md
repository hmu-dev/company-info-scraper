# ğŸ•µï¸â€â™‚ï¸ AI Web Scraper - Split API

A powerful AI-driven web scraping tool with **ultra-fast text extraction** and **paginated media processing**. Built with FastAPI for robust API access and AWS Lambda serverless deployment.

## âœ¨ Features

- **âš¡ Ultra-Fast Text**: Split endpoints for lightning-fast text extraction (0.2-0.3s)
- **ğŸ“¸ Smart Pagination**: Cursor-based pagination for efficient media loading
- **ğŸ§  AI Enhancement**: Optional AI-powered content analysis when needed
- **ğŸš€ FastAPI Service**: High-performance API service with automatic documentation
- **ğŸ” Smart Navigation**: Automatically finds relevant "About Us" pages
- **ğŸ“Š Structured Output**: Organized company profiles with confidence scoring
- **ğŸŒ AWS Deployed**: Live at https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/
- **ğŸ“š Comprehensive Docs**: Interactive API documentation with examples

## ğŸ”¬ How It Works

The AI Web Scraper uses a **split API strategy** that separates text extraction from media processing for optimal performance:

### ğŸ¯ **Split API Strategy**

Our API offers specialized endpoints for different use cases:

#### âš¡ **`/scrape/text`** - **ULTRA-FAST TEXT**
- ğŸƒâ€â™‚ï¸ Pure programmatic extraction (0.2-0.3s)
- ğŸ“Š Company information and about pages
- ğŸ’° Most cost-effective for text-only needs
- ğŸ¯ Perfect for initial app loading

#### ğŸ“¸ **`/scrape/media`** - **PAGINATED MEDIA**
- ğŸ”„ Cursor-based pagination for infinite scroll
- ğŸ–¼ï¸ Images, videos, documents, icons
- ğŸ“Š Smart prioritization (logos first)
- âš¡ Progressive loading support

#### ğŸ§  **`/scrape/enhance`** - **AI ENHANCEMENT**
- ğŸ§  AI-powered content analysis when needed
- ğŸ“ˆ Enhanced insights and summaries
- ğŸ¯ Use when programmatic results are insufficient
- ğŸ’¡ Smart confidence scoring

#### ğŸ”„ **`/scrape` & `/scrape/about`** - **LEGACY**
- ğŸ“œ Backward compatibility endpoints
- ğŸ”— Redirect to `/scrape/text`

## ğŸš€ Quick Start

### ğŸŒ **Live API Usage**

**Base URL**: `https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/`

**Interactive Documentation**: [https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)

#### **Example API Calls:**

```bash
# Ultra-fast text extraction
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/text?url=github.com"

# Paginated media extraction
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/media?url=github.com&limit=10"

# Next page of media (using cursor)
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/media?url=github.com&cursor=bWVkaWE6MTA=&limit=10"

# AI enhancement when needed
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/enhance?url=github.com"

# Health check
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/health"
```

#### **Python Example:**

```python
import requests
import base64

# Ultra-fast text extraction
text_response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/text",
    params={"url": "github.com"}
)

text_data = text_response.json()
print(f"Company: {text_data['title']}")
print(f"Description: {text_data['description']}")
print(f"About pages found: {text_data['about_pages_found']}")

# Paginated media extraction
media_response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/media",
    params={"url": "github.com", "limit": 10}
)

media_data = media_response.json()
print(f"Media assets: {len(media_data['media_assets'])}")
print(f"Total assets: {media_data['media_summary']['total_assets']}")

# Get next page if available
if media_data['pagination']['has_more']:
    next_cursor = media_data['pagination']['next_cursor']
    next_page = requests.get(
        "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/media",
        params={"url": "github.com", "cursor": next_cursor, "limit": 10}
    )
    print(f"Next page: {len(next_page.json()['media_assets'])} items")

# AI enhancement when needed
enhance_response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/enhance",
    params={"url": "github.com"}
)

enhance_data = enhance_response.json()
print(f"Enhanced insights: {enhance_data['ai_insights']}")
```

## ğŸš€ Local Development

### Prerequisites

- Python 3.9+
- AWS CLI (for deployment)
- AWS SAM CLI (for local testing)

### Installation

1. **Clone the project**

   ```bash
   git clone https://github.com/hmu-dev/company-info-scraper.git
   cd company-info-scraper
   ```

2. **Set up virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements-api.txt
   ```

4. **Local testing with SAM**

   ```bash
   cd about-us-scraper-service
   sam build --use-container
   sam local start-api
   ```

   Access at: `http://localhost:3000`

### Running the Application

#### Option 1: Local SAM Development

```bash
cd about-us-scraper-service
sam local start-api
```

- API: `http://localhost:3000`
- Docs: `http://localhost:3000/docs`
- Health: `http://localhost:3000/health`

#### Option 2: Direct FastAPI (Development)

```bash
cd about-us-scraper-service/api
python main_hybrid.py
```

- API: `http://localhost:8000`
- Docs: `http://localhost:8000/docs`

#### Option 3: Docker (API Only)

```bash
# Build and run
docker build -f Dockerfile.api -t ai-scraper-api .
docker run -p 8000:8000 ai-scraper-api
```

## ğŸ“– Usage

### API Endpoints

#### ğŸ§  **Intelligent Scraping** (Recommended)

```bash
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent?url=github.com&include_media=true"
```

**Features:**
- Auto-discovers About Us pages
- Extracts company info and media assets
- Uses AI enhancement when needed
- Confidence scoring

#### âš¡ **Fast Scraping** (Speed Optimized)

```bash
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/fast?url=example.com&include_media=true"
```

**Features:**
- Pure programmatic extraction
- Fastest response times (0.2-0.3s)
- No AI costs
- Good for high-volume requests

#### ğŸ”„ **Legacy Endpoints**

```bash
# Basic scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape?url=company.com"

# About page scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/about?url=company.com/about"
```

### Python Usage

```python
import requests

# Intelligent scraping with media
response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent",
    params={
        "url": "github.com",
        "include_media": True,
        "max_about_pages": 3
    }
)

result = response.json()
print(f"Company: {result['title']}")
print(f"Media assets: {result['media_summary']['total_assets']}")
print(f"Processing time: {result['processing_time_seconds']}s")
```

## ğŸ“ Project Structure

```
ai-web-scraper/
â”œâ”€â”€ about-us-scraper-service/   # Main AWS SAM service directory
â”‚   â”œâ”€â”€ api/                   # FastAPI application code
â”‚   â”‚   â”œâ”€â”€ main_hybrid.py     # Hybrid API implementation
â”‚   â”‚   â”œâ”€â”€ lambda_handler_hybrid.py # AWS Lambda handler
â”‚   â”‚   â”œâ”€â”€ endpoints/         # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ middleware/        # Request/response middleware
â”‚   â”‚   â””â”€â”€ utils/             # Helper functions
â”‚   â”œâ”€â”€ template-simple.yaml  # AWS SAM template
â”‚   â”œâ”€â”€ samconfig.toml        # SAM configuration
â”‚   â”œâ”€â”€ requirements.txt      # Lambda dependencies
â”‚   â””â”€â”€ docs/                 # Service documentation
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines
â”‚   â”œâ”€â”€ ci.yml               # Continuous integration
â”‚   â””â”€â”€ deploy.yml           # AWS deployment
â”œâ”€â”€ docs/                    # Project documentation
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md  # AWS deployment guide
â”‚   â”œâ”€â”€ API_README.md        # API documentation
â”‚   â””â”€â”€ PUBLIC_HOSTING_GUIDE.md # Hosting options
â”œâ”€â”€ tests/                   # Testing scripts
â”œâ”€â”€ requirements-api.txt     # API dependencies
â”œâ”€â”€ README.md               # This file
â””â”€â”€ .gitignore             # Git ignore rules
```

## ğŸŒ Deployment

### AWS Lambda (Production)

The API is deployed on AWS Lambda using SAM (Serverless Application Model):

- **Live URL**: https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/
- **Region**: us-east-1
- **Auto-deployment**: GitHub Actions CI/CD pipeline
- **Monitoring**: CloudWatch dashboards and alerts

### Local Development

```bash
cd about-us-scraper-service
sam build --use-container
sam local start-api
```

### Manual Deployment

```bash
cd about-us-scraper-service
sam build --use-container
sam deploy --guided
```

## ğŸ§ª Testing

### Test the Live API

```bash
# Health check
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/health"

# Intelligent scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent?url=github.com"

# Fast scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/fast?url=example.com"
```

### Local Testing

```bash
cd about-us-scraper-service
sam local invoke ScraperFunction -e events/test_event.json
```

## ğŸ“š Documentation

- **Interactive API Docs**: [https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)
- **Deployment Guide**: `docs/DEPLOYMENT_GUIDE.md`
- **API Documentation**: `docs/API_README.md`
- **Development Guide**: `about-us-scraper-service/docs/DEVELOPMENT.md`

## ğŸ¤ Contributing

This project was developed through collaborative AI-assisted programming. The codebase includes:

- Hybrid intelligence approach (programmatic + AI)
- Robust error handling and fallback mechanisms
- Comprehensive media extraction and processing
- Smart website navigation and About page discovery
- AWS serverless deployment with monitoring

## ğŸ“„ License

This project is open source and available under standard open source licensing.

## ğŸ†˜ Support

For issues or questions:

1. Check the [live API documentation](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)
2. Review the test scripts for usage examples
3. Examine the project documentation files

---

**Built with â¤ï¸ using AI-assisted development**
