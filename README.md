# ğŸ•µï¸â€â™‚ï¸ AI Web Scraper

A powerful AI-driven web scraping tool that extracts comprehensive company profiles including text content and media files. Built with FastAPI for robust API access and AWS Lambda serverless deployment.

## âœ¨ Features

- **ğŸ§  Hybrid Intelligence**: Combines fast programmatic extraction with smart AI enhancement
- **ğŸš€ FastAPI Service**: High-performance API service with automatic documentation
- **âš¡ Speed Optimized**: Fast endpoints (0.2-0.3s) for high-volume requests
- **ğŸ–¼ï¸ Media Extraction**: Downloads and processes images, videos, and documents
- **ğŸ” Smart Navigation**: Automatically finds relevant "About Us" pages
- **ğŸ“Š Structured Output**: Organized company profiles with confidence scoring
- **ğŸŒ AWS Deployed**: Live at https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/
- **ğŸ“š Comprehensive Docs**: Interactive API documentation with examples

## ğŸ”¬ How It Works

The AI Web Scraper uses a **hybrid approach** that combines the speed of programmatic extraction with the intelligence of AI analysis:

### ğŸ¯ **Hybrid Intelligence Strategy**

Our API offers multiple endpoints optimized for different use cases:

#### ğŸ§  **`/scrape/intelligent`** - **RECOMMENDED**
- âš¡ Starts with fast programmatic extraction
- ğŸ§  Falls back to AI when results are poor
- ğŸ” Auto-discovers About Us pages
- ğŸ“¸ Extracts all media assets
- ğŸ¯ Perfect for comprehensive company analysis

#### âš¡ **`/scrape/fast`** - **SPEED FOCUSED**
- ğŸƒâ€â™‚ï¸ Pure programmatic approach (no AI)
- âš¡ Fastest response times (0.2-0.3s)
- ğŸ“Š Good for basic company info
- ğŸ’° Most cost-effective

#### ğŸ”„ **`/scrape` & `/scrape/about`** - **LEGACY**
- ğŸ“œ Simple programmatic extraction
- ğŸ”— Use for existing integrations

## ğŸš€ Quick Start

### ğŸŒ **Live API Usage**

**Base URL**: `https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/`

**Interactive Documentation**: [https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)

#### **Example API Calls:**

```bash
# Intelligent scraping (recommended)
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent?url=github.com&include_media=true"

# Fast scraping (speed optimized)
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/fast?url=example.com&include_media=true"

# Health check
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/health"
```

#### **Python Example:**

```python
import requests

# Intelligent scraping with media
response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent",
    params={
        "url": "github.com",
        "include_media": True,
        "max_about_pages": 3
    }
)

result = response.json()
print(f"Company: {result['title']}")
print(f"Media assets: {result['media_summary']['total_assets']}")
print(f"Processing time: {result['processing_time_seconds']}s")
```

## ğŸš€ Local Development

### Prerequisites

- Python 3.9+
- AWS CLI (for deployment)
- AWS SAM CLI (for local testing)

### Installation

1. **Clone the project**

   ```bash
   git clone https://github.com/hmu-dev/company-info-scraper.git
   cd company-info-scraper
   ```

2. **Set up virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements-api.txt
   ```

4. **Local testing with SAM**

   ```bash
   cd about-us-scraper-service
   sam build --use-container
   sam local start-api
   ```

   Access at: `http://localhost:3000`

### Running the Application

#### Option 1: Local SAM Development

```bash
cd about-us-scraper-service
sam local start-api
```

- API: `http://localhost:3000`
- Docs: `http://localhost:3000/docs`
- Health: `http://localhost:3000/health`

#### Option 2: Direct FastAPI (Development)

```bash
cd about-us-scraper-service/api
python main_hybrid.py
```

- API: `http://localhost:8000`
- Docs: `http://localhost:8000/docs`

#### Option 3: Docker (API Only)

```bash
# Build and run
docker build -f Dockerfile.api -t ai-scraper-api .
docker run -p 8000:8000 ai-scraper-api
```

## ğŸ“– Usage

### API Endpoints

#### ğŸ§  **Intelligent Scraping** (Recommended)

```bash
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent?url=github.com&include_media=true"
```

**Features:**
- Auto-discovers About Us pages
- Extracts company info and media assets
- Uses AI enhancement when needed
- Confidence scoring

#### âš¡ **Fast Scraping** (Speed Optimized)

```bash
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/fast?url=example.com&include_media=true"
```

**Features:**
- Pure programmatic extraction
- Fastest response times (0.2-0.3s)
- No AI costs
- Good for high-volume requests

#### ğŸ”„ **Legacy Endpoints**

```bash
# Basic scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape?url=company.com"

# About page scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/about?url=company.com/about"
```

### Python Usage

```python
import requests

# Intelligent scraping with media
response = requests.get(
    "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent",
    params={
        "url": "github.com",
        "include_media": True,
        "max_about_pages": 3
    }
)

result = response.json()
print(f"Company: {result['title']}")
print(f"Media assets: {result['media_summary']['total_assets']}")
print(f"Processing time: {result['processing_time_seconds']}s")
```

## ğŸ“ Project Structure

```
ai-web-scraper/
â”œâ”€â”€ about-us-scraper-service/   # Main AWS SAM service directory
â”‚   â”œâ”€â”€ api/                   # FastAPI application code
â”‚   â”‚   â”œâ”€â”€ main_hybrid.py     # Hybrid API implementation
â”‚   â”‚   â”œâ”€â”€ lambda_handler_hybrid.py # AWS Lambda handler
â”‚   â”‚   â”œâ”€â”€ endpoints/         # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ middleware/        # Request/response middleware
â”‚   â”‚   â””â”€â”€ utils/             # Helper functions
â”‚   â”œâ”€â”€ template-simple.yaml  # AWS SAM template
â”‚   â”œâ”€â”€ samconfig.toml        # SAM configuration
â”‚   â”œâ”€â”€ requirements.txt      # Lambda dependencies
â”‚   â””â”€â”€ docs/                 # Service documentation
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines
â”‚   â”œâ”€â”€ ci.yml               # Continuous integration
â”‚   â””â”€â”€ deploy.yml           # AWS deployment
â”œâ”€â”€ docs/                    # Project documentation
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md  # AWS deployment guide
â”‚   â”œâ”€â”€ API_README.md        # API documentation
â”‚   â””â”€â”€ PUBLIC_HOSTING_GUIDE.md # Hosting options
â”œâ”€â”€ tests/                   # Testing scripts
â”œâ”€â”€ requirements-api.txt     # API dependencies
â”œâ”€â”€ README.md               # This file
â””â”€â”€ .gitignore             # Git ignore rules
```

## ğŸŒ Deployment

### AWS Lambda (Production)

The API is deployed on AWS Lambda using SAM (Serverless Application Model):

- **Live URL**: https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/
- **Region**: us-east-1
- **Auto-deployment**: GitHub Actions CI/CD pipeline
- **Monitoring**: CloudWatch dashboards and alerts

### Local Development

```bash
cd about-us-scraper-service
sam build --use-container
sam local start-api
```

### Manual Deployment

```bash
cd about-us-scraper-service
sam build --use-container
sam deploy --guided
```

## ğŸ§ª Testing

### Test the Live API

```bash
# Health check
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/health"

# Intelligent scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/intelligent?url=github.com"

# Fast scraping
curl "https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/scrape/fast?url=example.com"
```

### Local Testing

```bash
cd about-us-scraper-service
sam local invoke ScraperFunction -e events/test_event.json
```

## ğŸ“š Documentation

- **Interactive API Docs**: [https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)
- **Deployment Guide**: `docs/DEPLOYMENT_GUIDE.md`
- **API Documentation**: `docs/API_README.md`
- **Development Guide**: `about-us-scraper-service/docs/DEVELOPMENT.md`

## ğŸ¤ Contributing

This project was developed through collaborative AI-assisted programming. The codebase includes:

- Hybrid intelligence approach (programmatic + AI)
- Robust error handling and fallback mechanisms
- Comprehensive media extraction and processing
- Smart website navigation and About page discovery
- AWS serverless deployment with monitoring

## ğŸ“„ License

This project is open source and available under standard open source licensing.

## ğŸ†˜ Support

For issues or questions:

1. Check the [live API documentation](https://cjp6f8947h.execute-api.us-east-1.amazonaws.com/docs)
2. Review the test scripts for usage examples
3. Examine the project documentation files

---

**Built with â¤ï¸ using AI-assisted development**
